{"cells":[{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-3372342c-ec8d-4d5e-8bbe-0730dacfc5d6","output_cleared":false,"source_hash":"1520b428","execution_millis":0,"execution_start":1606593396157},"source":"import xml.sax\nimport re\nfrom collections import defaultdict, OrderedDict\nfrom os import path, listdir\nimport sys\nimport pickle\nimport time\nimport heapq\nimport json, nltk\nimport pandas as pd\nfrom collections import defaultdict\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00001-dd919c1b-9c95-4447-bba1-1a429f38bc64","output_cleared":false,"source_hash":"92085fa0","execution_millis":0,"execution_start":1606593396157},"source":"#Need to change these below\nINPUT_FILE = '/datasets/wiki-dump/wiki-pages/'\nTITLES = '/datasets/wiki-dump/title_index/'\nBODY = '/datasets/wiki-dump/body_index/'\n# INDEX_STAT = '​invertedindex_stat.txt​'\nTOTAL_TOKENS = 0\nTOTAL_INV_TOKENS = 0\nindexMap = defaultdict(list)\nfile_num = 0\npages = 1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00002-4349926f-4ad7-4504-af0f-7fce80bfa640","output_cleared":false,"source_hash":"1eacb011","execution_millis":0,"execution_start":1606593396158},"source":"def split_titles(txt):\n    return txt.split('_')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-4186b3e8-bc72-44d6-adf5-baf94ee8b6c7","output_cleared":false,"source_hash":"f21a39a2","execution_millis":17,"execution_start":1606593396159},"source":"def body_index(sentences, fstr, n):\n    res = defaultdict(list)\n    itr = 0\n    for article in sentences:\n        n_lines = article.split('\\n')[:n]\n        lst = []\n        d = defaultdict(lambda: 0)\n        for line in n_lines:\n            temp = line.split()[1:]\n            for j in temp:\n                d[j] += 1\n        for word in d.keys():\n            string = fstr + '-' + str(itr) + '-' + str(d[word])\n            res[word].append(string)\n        itr += 1\n    del sentences\n    return res","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00002-89795a88-8981-4fbe-b3db-ba14e8343de8","output_cleared":false,"source_hash":"1cd70011","execution_millis":0,"execution_start":1606593396177},"source":"def title_index(titles, fstr):\n    res = defaultdict(list)\n    itr = 0\n    for lst in titles:\n        lst = lst.split('_')\n        ln = 0\n        for word in lst:\n            if word.startswith('-LRB'):\n                ln += 0.5\n            else:\n                ln += 1\n        # print(ln)\n        d = defaultdict(lambda: 0)\n        for j in lst:\n            d[j] += 1\n        for word in d.keys():\n            string = fstr + '-' + str(itr) + '-' + str(d[word]) + '-' + str(ln)\n            # print(string)\n            res[word].append(string)\n        itr += 1\n    del titles\n    return res","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-3ad995f4-785f-4a25-9c05-f1dc12441bde","output_cleared":false,"source_hash":"15449369","execution_millis":0,"execution_start":1606593396177},"source":"def store_index(ind, st, titles = True):\n    index_map_file = []\n    for key in sorted(ind.keys()):\n        string = key + ':' + ' '.join(ind[key])\n        index_map_file.append(string)\n    if titles:\n        with open(TITLES+st+'.txt' ,\"w+\") as f:\n            f.write('\\n'.join(index_map_file))\n    else:\n        with open(BODY+st+'.txt' ,\"w+\") as f:\n            f.write('\\n'.join(index_map_file))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00002-163282b7-0322-4859-82dc-47d814044f4d","output_cleared":false,"source_hash":"6788a1e6","execution_millis":40,"execution_start":1606593396178},"source":"def wiki_parse():\n    print(\"Starting parser\")\n    file_num = 0\n    total_time = 0\n    for i in tqdm(range(1,110)):\n        if i < 10:\n            fstr = '00' + str(i)\n        elif i < 100:\n            fstr = '0' + str(i)\n        else:\n            fstr = str(i)\n        train = []\n        raw = open('/datasets/wiki-dump/wiki-pages/wiki-'+fstr+'.jsonl', 'r')\n        for line in raw:\n            obj = json.loads(line)\n            train.append(obj)\n        raw.close()\n        df_wiki = pd.DataFrame(train)\n        del train\n        title_ind = title_index(df_wiki.id, fstr)\n        store_index(title_ind, str(fstr))\n        del title_ind\n        body_ind = body_index(df_wiki.lines, fstr, 2)\n        store_index(body_ind, str(fstr), titles=False)\n        del body_ind\n        del df_wiki\n        # parse_start_time = time.time()\n        # xml_parser = xml.sax.make_parser()\n        # xml_parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n        # handler = xml_handler(parse_start_time, file_num)\n        # xml_parser.setContentHandler(handler)\n        # xml_parser.parse(INPUT_FILE+'/'+filename)\n        # print(\"Parsing finished\")\n        # # store_index(file_num)\n        # # print(\"Dumping finished\")\n        # time_taken = time.time() - parse_start_time\n        # total_time += time_taken\n        # print(\"Time taken: \",time_taken)\n        # print(\"Time elapsed: \",total_time)\n        # print(\"Pages: \",pages)\n        # total_time += time_taken\n        # indexMap = defaultdict(list)\n        # file_num += 1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-9a9ef4e8-c842-4e73-9690-193ae2f06bff","output_cleared":false,"source_hash":"efec26f4","execution_millis":682193,"execution_start":1606593396218},"source":"wiki_parse()","execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/109 [00:00<?, ?it/s]Starting parser\n100%|██████████| 109/109 [11:22<00:00,  6.26s/it]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00006-ffb5408a-3149-4b56-a2b0-c0426e102a45","output_cleared":false,"source_hash":"e8fa9bc7","execution_millis":4,"execution_start":1606594078429},"source":"def merge_index_titles():\n    global pages\n    file_pointers = []\n    for filename in listdir(TITLES):\n        if filename.endswith('.txt'):\n            print(TITLES+filename)\n            file_pointers.append(open(TITLES+filename, 'r'))\n    # ind = open('index/index','w+')\n    # off = open('index/offset','w+')\n    file_pointers_temp = file_pointers[0:40]\n    last_file_count = 40\n    file_pointers_flag = 1\n    loop_count = 0\n    while file_pointers_flag:\n        print(last_file_count)\n        wordpostings = defaultdict(lambda: [])\n        words = {}\n        heap = []\n        wordfilemap = defaultdict(lambda: [])\n        curline = {}\n        finishflag = 1\n        flag = 0\n        filecomplete = [0 for i in range(len(file_pointers_temp))]\n        ind = open(TITLES+'index_temp'+str(loop_count),'w+')\n        off = open(TITLES+'offset_temp'+str(loop_count),'w+')\n        for i in range(len(file_pointers_temp)):\n            curline[i] = file_pointers_temp[i].readline().strip()\n            # print(curline[i])\n            word = curline[i].split(':')[0]\n            wordfilemap[word].append(i)\n            wordpostings[word] += curline[i].split(':')[1].split(\" \")\n            if word not in heap:\n                heapq.heappush(heap,word)\n        while (finishflag):\n            minword = heapq.heappop(heap)\n            string = minword + ':' + str(ind.tell())\n            string = string.strip() + '\\n'\n            off.write(string)\n            del string\n            string = minword + \":\" + \" \".join(wordpostings[minword]) + \"\\n\"\n            ind.write(string)\n            del string\n            filenum = wordfilemap[minword]\n            # print(wordfilemap)\n            wordfilemap.pop(minword)\n            wordpostings.pop(minword)\n            for num in filenum:\n                nextline = file_pointers_temp[num].readline().strip()\n                if nextline == '':\n                    filecomplete[num] = 1\n                else:\n                    newword = nextline.split(':')[0]\n                    wordpostings[newword] += nextline.split(':')[1].split(\" \")\n                    # print(wordfilemap[newword])\n                    if not wordfilemap[newword]:\n                        heapq.heappush(heap,newword)\n                        wordfilemap[newword].append(num)\n                    else:\n                        wordfilemap[newword].append(num)\n            for i in range(len(file_pointers_temp)):\n                flag = filecomplete[i] + flag\n            flag = int(flag/(len(file_pointers_temp)))\n            if flag==1:\n                finishflag = 0\n        for i in range(len(file_pointers_temp)):\n            file_pointers_temp[i].close()\n        ind.close()\n        off.close()\n        if file_pointers_flag==2:\n            file_pointers_flag = 0\n        elif last_file_count+39 >= len(file_pointers):\n            file_pointers_temp = file_pointers[last_file_count:]\n            file_pointers_flag = 2\n        else:\n            file_pointers_temp = file_pointers[last_file_count:last_file_count+39]\n        file_pointers_temp.append(open(TITLES+'index_temp'+str(loop_count),'r'))\n        last_file_count = last_file_count + 39\n        loop_count += 1\n        wordpostings = None\n        words = None\n        heap = None\n        wordfilemap = None\n        curline = None\n    return","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00009-c67699de-7cbf-4feb-b2aa-6abf56e7d191","output_cleared":false,"source_hash":"9611d32d","execution_millis":81359,"execution_start":1606594078435},"source":"merge_index_titles()","execution_count":null,"outputs":[{"name":"stdout","text":"/datasets/wiki-dump/title_index/001.txt\n/datasets/wiki-dump/title_index/002.txt\n/datasets/wiki-dump/title_index/003.txt\n/datasets/wiki-dump/title_index/004.txt\n/datasets/wiki-dump/title_index/005.txt\n/datasets/wiki-dump/title_index/006.txt\n/datasets/wiki-dump/title_index/007.txt\n/datasets/wiki-dump/title_index/008.txt\n/datasets/wiki-dump/title_index/009.txt\n/datasets/wiki-dump/title_index/010.txt\n/datasets/wiki-dump/title_index/011.txt\n/datasets/wiki-dump/title_index/012.txt\n/datasets/wiki-dump/title_index/013.txt\n/datasets/wiki-dump/title_index/014.txt\n/datasets/wiki-dump/title_index/015.txt\n/datasets/wiki-dump/title_index/016.txt\n/datasets/wiki-dump/title_index/017.txt\n/datasets/wiki-dump/title_index/018.txt\n/datasets/wiki-dump/title_index/019.txt\n/datasets/wiki-dump/title_index/020.txt\n/datasets/wiki-dump/title_index/021.txt\n/datasets/wiki-dump/title_index/022.txt\n/datasets/wiki-dump/title_index/023.txt\n/datasets/wiki-dump/title_index/024.txt\n/datasets/wiki-dump/title_index/025.txt\n/datasets/wiki-dump/title_index/026.txt\n/datasets/wiki-dump/title_index/027.txt\n/datasets/wiki-dump/title_index/028.txt\n/datasets/wiki-dump/title_index/029.txt\n/datasets/wiki-dump/title_index/030.txt\n/datasets/wiki-dump/title_index/031.txt\n/datasets/wiki-dump/title_index/032.txt\n/datasets/wiki-dump/title_index/033.txt\n/datasets/wiki-dump/title_index/034.txt\n/datasets/wiki-dump/title_index/035.txt\n/datasets/wiki-dump/title_index/036.txt\n/datasets/wiki-dump/title_index/037.txt\n/datasets/wiki-dump/title_index/038.txt\n/datasets/wiki-dump/title_index/039.txt\n/datasets/wiki-dump/title_index/040.txt\n/datasets/wiki-dump/title_index/041.txt\n/datasets/wiki-dump/title_index/042.txt\n/datasets/wiki-dump/title_index/043.txt\n/datasets/wiki-dump/title_index/044.txt\n/datasets/wiki-dump/title_index/045.txt\n/datasets/wiki-dump/title_index/046.txt\n/datasets/wiki-dump/title_index/047.txt\n/datasets/wiki-dump/title_index/048.txt\n/datasets/wiki-dump/title_index/049.txt\n/datasets/wiki-dump/title_index/050.txt\n/datasets/wiki-dump/title_index/051.txt\n/datasets/wiki-dump/title_index/052.txt\n/datasets/wiki-dump/title_index/053.txt\n/datasets/wiki-dump/title_index/054.txt\n/datasets/wiki-dump/title_index/055.txt\n/datasets/wiki-dump/title_index/056.txt\n/datasets/wiki-dump/title_index/057.txt\n/datasets/wiki-dump/title_index/058.txt\n/datasets/wiki-dump/title_index/059.txt\n/datasets/wiki-dump/title_index/060.txt\n/datasets/wiki-dump/title_index/061.txt\n/datasets/wiki-dump/title_index/062.txt\n/datasets/wiki-dump/title_index/063.txt\n/datasets/wiki-dump/title_index/064.txt\n/datasets/wiki-dump/title_index/065.txt\n/datasets/wiki-dump/title_index/066.txt\n/datasets/wiki-dump/title_index/067.txt\n/datasets/wiki-dump/title_index/068.txt\n/datasets/wiki-dump/title_index/069.txt\n/datasets/wiki-dump/title_index/070.txt\n/datasets/wiki-dump/title_index/071.txt\n/datasets/wiki-dump/title_index/072.txt\n/datasets/wiki-dump/title_index/073.txt\n/datasets/wiki-dump/title_index/074.txt\n/datasets/wiki-dump/title_index/075.txt\n/datasets/wiki-dump/title_index/076.txt\n/datasets/wiki-dump/title_index/077.txt\n/datasets/wiki-dump/title_index/078.txt\n/datasets/wiki-dump/title_index/079.txt\n/datasets/wiki-dump/title_index/080.txt\n/datasets/wiki-dump/title_index/081.txt\n/datasets/wiki-dump/title_index/082.txt\n/datasets/wiki-dump/title_index/083.txt\n/datasets/wiki-dump/title_index/084.txt\n/datasets/wiki-dump/title_index/085.txt\n/datasets/wiki-dump/title_index/086.txt\n/datasets/wiki-dump/title_index/087.txt\n/datasets/wiki-dump/title_index/088.txt\n/datasets/wiki-dump/title_index/089.txt\n/datasets/wiki-dump/title_index/090.txt\n/datasets/wiki-dump/title_index/091.txt\n/datasets/wiki-dump/title_index/092.txt\n/datasets/wiki-dump/title_index/093.txt\n/datasets/wiki-dump/title_index/094.txt\n/datasets/wiki-dump/title_index/095.txt\n/datasets/wiki-dump/title_index/096.txt\n/datasets/wiki-dump/title_index/097.txt\n/datasets/wiki-dump/title_index/098.txt\n/datasets/wiki-dump/title_index/099.txt\n/datasets/wiki-dump/title_index/100.txt\n/datasets/wiki-dump/title_index/101.txt\n/datasets/wiki-dump/title_index/102.txt\n/datasets/wiki-dump/title_index/103.txt\n/datasets/wiki-dump/title_index/104.txt\n/datasets/wiki-dump/title_index/105.txt\n/datasets/wiki-dump/title_index/106.txt\n/datasets/wiki-dump/title_index/107.txt\n/datasets/wiki-dump/title_index/108.txt\n/datasets/wiki-dump/title_index/109.txt\n40\n79\n118\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00010-70e5310c-8180-458c-bcb7-8cc5d73d81e3","output_cleared":false,"source_hash":"89eeb6ee","execution_millis":0,"execution_start":1606594159794},"source":"def merge_index_body():\n    global pages\n    file_pointers = []\n    for filename in listdir(BODY):\n        if filename.endswith('.txt'):\n            print(BODY+filename)\n            file_pointers.append(open(BODY+filename, 'r'))\n    # ind = open('index/index','w+')\n    # off = open('index/offset','w+')\n    file_pointers_temp = file_pointers[0:40]\n    last_file_count = 40\n    file_pointers_flag = 1\n    loop_count = 0\n    while file_pointers_flag:\n        print(last_file_count)\n        wordpostings = defaultdict(lambda: [])\n        words = {}\n        heap = []\n        wordfilemap = defaultdict(lambda: [])\n        curline = {}\n        finishflag = 1\n        flag = 0\n        filecomplete = [0 for i in range(len(file_pointers_temp))]\n        ind = open(BODY+'index_temp'+str(loop_count),'w+')\n        off = open(BODY+'offset_temp'+str(loop_count),'w+')\n        for i in range(len(file_pointers_temp)):\n            curline[i] = file_pointers_temp[i].readline().strip()\n            # print(curline[i])\n            word = curline[i].split(':')[0]\n            wordfilemap[word].append(i)\n            wordpostings[word] += curline[i].split(':')[1].split(\" \")\n            if word not in heap:\n                heapq.heappush(heap,word)\n        while (finishflag):\n            minword = heapq.heappop(heap)\n            string = minword + ':' + str(ind.tell())\n            string = string.strip() + '\\n'\n            off.write(string)\n            del string\n            string = minword + \":\" + \" \".join(wordpostings[minword]) + \"\\n\"\n            ind.write(string)\n            del string\n            filenum = wordfilemap[minword]\n            # print(wordfilemap)\n            wordfilemap.pop(minword)\n            wordpostings.pop(minword)\n            for num in filenum:\n                nextline = file_pointers_temp[num].readline().strip()\n                if nextline == '':\n                    filecomplete[num] = 1\n                else:\n                    newword = nextline.split(':')[0]\n                    wordpostings[newword] += nextline.split(':')[1].split(\" \")\n                    # print(wordfilemap[newword])\n                    if not wordfilemap[newword]:\n                        heapq.heappush(heap,newword)\n                        wordfilemap[newword].append(num)\n                    else:\n                        wordfilemap[newword].append(num)\n            for i in range(len(file_pointers_temp)):\n                flag = filecomplete[i] + flag\n            flag = int(flag/(len(file_pointers_temp)))\n            if flag==1:\n                finishflag = 0\n        for i in range(len(file_pointers_temp)):\n            file_pointers_temp[i].close()\n        ind.close()\n        off.close()\n        if file_pointers_flag==2:\n            file_pointers_flag = 0\n        elif last_file_count+39 >= len(file_pointers):\n            file_pointers_temp = file_pointers[last_file_count:]\n            file_pointers_flag = 2\n        else:\n            file_pointers_temp = file_pointers[last_file_count:last_file_count+39]\n        file_pointers_temp.append(open(BODY+'index_temp'+str(loop_count),'r'))\n        last_file_count = last_file_count + 39\n        loop_count += 1\n        wordpostings = None\n        words = None\n        heap = None\n        wordfilemap = None\n        curline = None\n    return","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00011-3be63c1f-2d42-41af-b308-39147920124e","output_cleared":false,"source_hash":"da76a08c","execution_millis":247523,"execution_start":1606594159795},"source":"merge_index_body()","execution_count":null,"outputs":[{"name":"stdout","text":"/datasets/wiki-dump/body_index/001.txt\n/datasets/wiki-dump/body_index/002.txt\n/datasets/wiki-dump/body_index/003.txt\n/datasets/wiki-dump/body_index/004.txt\n/datasets/wiki-dump/body_index/005.txt\n/datasets/wiki-dump/body_index/006.txt\n/datasets/wiki-dump/body_index/007.txt\n/datasets/wiki-dump/body_index/008.txt\n/datasets/wiki-dump/body_index/009.txt\n/datasets/wiki-dump/body_index/010.txt\n/datasets/wiki-dump/body_index/011.txt\n/datasets/wiki-dump/body_index/012.txt\n/datasets/wiki-dump/body_index/013.txt\n/datasets/wiki-dump/body_index/014.txt\n/datasets/wiki-dump/body_index/015.txt\n/datasets/wiki-dump/body_index/016.txt\n/datasets/wiki-dump/body_index/017.txt\n/datasets/wiki-dump/body_index/018.txt\n/datasets/wiki-dump/body_index/019.txt\n/datasets/wiki-dump/body_index/020.txt\n/datasets/wiki-dump/body_index/021.txt\n/datasets/wiki-dump/body_index/022.txt\n/datasets/wiki-dump/body_index/023.txt\n/datasets/wiki-dump/body_index/024.txt\n/datasets/wiki-dump/body_index/025.txt\n/datasets/wiki-dump/body_index/026.txt\n/datasets/wiki-dump/body_index/027.txt\n/datasets/wiki-dump/body_index/028.txt\n/datasets/wiki-dump/body_index/029.txt\n/datasets/wiki-dump/body_index/030.txt\n/datasets/wiki-dump/body_index/031.txt\n/datasets/wiki-dump/body_index/032.txt\n/datasets/wiki-dump/body_index/033.txt\n/datasets/wiki-dump/body_index/034.txt\n/datasets/wiki-dump/body_index/035.txt\n/datasets/wiki-dump/body_index/036.txt\n/datasets/wiki-dump/body_index/037.txt\n/datasets/wiki-dump/body_index/038.txt\n/datasets/wiki-dump/body_index/039.txt\n/datasets/wiki-dump/body_index/040.txt\n/datasets/wiki-dump/body_index/041.txt\n/datasets/wiki-dump/body_index/042.txt\n/datasets/wiki-dump/body_index/043.txt\n/datasets/wiki-dump/body_index/044.txt\n/datasets/wiki-dump/body_index/045.txt\n/datasets/wiki-dump/body_index/046.txt\n/datasets/wiki-dump/body_index/047.txt\n/datasets/wiki-dump/body_index/048.txt\n/datasets/wiki-dump/body_index/049.txt\n/datasets/wiki-dump/body_index/050.txt\n/datasets/wiki-dump/body_index/051.txt\n/datasets/wiki-dump/body_index/052.txt\n/datasets/wiki-dump/body_index/053.txt\n/datasets/wiki-dump/body_index/054.txt\n/datasets/wiki-dump/body_index/055.txt\n/datasets/wiki-dump/body_index/056.txt\n/datasets/wiki-dump/body_index/057.txt\n/datasets/wiki-dump/body_index/058.txt\n/datasets/wiki-dump/body_index/059.txt\n/datasets/wiki-dump/body_index/060.txt\n/datasets/wiki-dump/body_index/061.txt\n/datasets/wiki-dump/body_index/062.txt\n/datasets/wiki-dump/body_index/063.txt\n/datasets/wiki-dump/body_index/064.txt\n/datasets/wiki-dump/body_index/065.txt\n/datasets/wiki-dump/body_index/066.txt\n/datasets/wiki-dump/body_index/067.txt\n/datasets/wiki-dump/body_index/068.txt\n/datasets/wiki-dump/body_index/069.txt\n/datasets/wiki-dump/body_index/070.txt\n/datasets/wiki-dump/body_index/071.txt\n/datasets/wiki-dump/body_index/072.txt\n/datasets/wiki-dump/body_index/073.txt\n/datasets/wiki-dump/body_index/074.txt\n/datasets/wiki-dump/body_index/075.txt\n/datasets/wiki-dump/body_index/076.txt\n/datasets/wiki-dump/body_index/077.txt\n/datasets/wiki-dump/body_index/078.txt\n/datasets/wiki-dump/body_index/079.txt\n/datasets/wiki-dump/body_index/080.txt\n/datasets/wiki-dump/body_index/081.txt\n/datasets/wiki-dump/body_index/082.txt\n/datasets/wiki-dump/body_index/083.txt\n/datasets/wiki-dump/body_index/084.txt\n/datasets/wiki-dump/body_index/085.txt\n/datasets/wiki-dump/body_index/086.txt\n/datasets/wiki-dump/body_index/087.txt\n/datasets/wiki-dump/body_index/088.txt\n/datasets/wiki-dump/body_index/089.txt\n/datasets/wiki-dump/body_index/090.txt\n/datasets/wiki-dump/body_index/091.txt\n/datasets/wiki-dump/body_index/092.txt\n/datasets/wiki-dump/body_index/093.txt\n/datasets/wiki-dump/body_index/094.txt\n/datasets/wiki-dump/body_index/095.txt\n/datasets/wiki-dump/body_index/096.txt\n/datasets/wiki-dump/body_index/097.txt\n/datasets/wiki-dump/body_index/098.txt\n/datasets/wiki-dump/body_index/099.txt\n/datasets/wiki-dump/body_index/100.txt\n/datasets/wiki-dump/body_index/101.txt\n/datasets/wiki-dump/body_index/102.txt\n/datasets/wiki-dump/body_index/103.txt\n/datasets/wiki-dump/body_index/104.txt\n/datasets/wiki-dump/body_index/105.txt\n/datasets/wiki-dump/body_index/106.txt\n/datasets/wiki-dump/body_index/107.txt\n/datasets/wiki-dump/body_index/108.txt\n/datasets/wiki-dump/body_index/109.txt\n40\n79\n118\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-75854889-bdab-4228-9f64-a42bc17fae3b","output_cleared":false,"source_hash":"b623e53d","execution_millis":1,"execution_start":1606594407318},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"a7b9c1ec-963a-43ef-99b6-1d06c97945af","deepnote_execution_queue":[]}}