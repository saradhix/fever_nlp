{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FEVER-InferBERT-sentence-transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms91KhQZMMMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae7b9d8-35af-474e-fb19-ce532afb8c15"
      },
      "source": [
        "#Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSXsvdAiair6",
        "outputId": "448420ed-be15-4e08-bcb4-1a4ad6390c94"
      },
      "source": [
        "import os\n",
        "drive='/content/gdrive/My Drive/python/fever/'  #Change the path to the correct directory where the cleaned_data.csv file is present\n",
        "os.chdir(drive)\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/python/fever\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ntzcrUW9PWn",
        "outputId": "ca6c866f-cae4-4667-df5e-64ae54463db8"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/5a/6e41e8383913dd2ba923cdcd02be2e03911595f4d2f9de559ecbed80d2d3/sentence-transformers-0.3.9.tar.gz (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 29.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 40kB 20.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51kB 17.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61kB 19.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: transformers<3.6.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.9.3)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.9-cp36-none-any.whl size=101036 sha256=2313c71d511a57dc5776f5eae75246300663632a85b9e33f8a0c700ad8ada7f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/89/43/f2f5bc00b03ef9724b0f6254a97eaf159a4c4ddc024b33e07a\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-0.3.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xac_jLolxfBE",
        "outputId": "158b1d52-6fcb-4ffe-dcd2-13926e54069b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 61.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=e8b67f7ef6bf25db59dc622b2c903e5e838fdfadfe5107f8e67c20c46bc0c1ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B89_DtQIvdZN",
        "outputId": "b821f047-16fb-4202-871d-cd0da8866b8b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "body_index\t\t\t    neural_network.py\n",
            "cache_dir\t\t\t    __pycache__\n",
            "combined_replaced_pronouns.jsonl    runs\n",
            "fever_bert_base_encased\t\t    tfidf_vectorizer_5000_features.pickle\n",
            "formatted_data_train_3_class.jsonl  title_index\n",
            "license.html\t\t\t    wiki-pages\n",
            "__MACOSX\t\t\t    wiki-pages.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0ab6PCq9UxT"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "def get_vectors(sentences):\n",
        "  model_name = 'bert-base-nli-cls-token'\n",
        "  model = SentenceTransformer(model_name)\n",
        "  embeddings = model.encode(sentences)\n",
        "  return embeddings \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBEHS19yx1Zx",
        "outputId": "09d4633a-3f5e-4c75-aa1e-d85579884bf5"
      },
      "source": [
        "print(get_vectors([\"This is a cat\", \"This is a dog\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.36742917 -0.1829594   0.8470633  ...  0.5516481   0.4876297\n",
            "   0.2861368 ]\n",
            " [ 0.39284614 -0.12082902  0.43790206 ... -0.71040785  0.16923594\n",
            "   0.6204951 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs5GDWGsbqgs"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import neural_network as nn\n",
        "def pre_process(sentence):\n",
        "    ##Replace brackets\n",
        "    brackets = ['-LRB-', '-LSB-', '-RRB-', '-RSB-']\n",
        "    for bracket in brackets:\n",
        "        sentence = sentence.replace(bracket, \" \")\n",
        "    return sentence\n",
        "\n",
        "def get_training_data():\n",
        "    samples_to_use = 250000  #max=368892\n",
        "    label_dict = {\"SUPPORTS\":0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
        "\n",
        "    X_all = []\n",
        "    y_all = []\n",
        "    count = 0\n",
        "    #train_file = \"formatted_data_train.jsonl\"\n",
        "    train_file = \"formatted_data_train_3_class.jsonl\"\n",
        "    fp = open(train_file, 'r')\n",
        "    for line in fp:\n",
        "        if count >= samples_to_use:\n",
        "            break\n",
        "        obj = json.loads(line.strip())\n",
        "        claim = pre_process(obj['claim'])\n",
        "        evidence = pre_process(obj['evidence'])\n",
        "        X = (claim, evidence)\n",
        "        y = label_dict[obj['label']]\n",
        "        X_all.append(X)\n",
        "        y_all.append(y)\n",
        "        count += 1\n",
        "    return X_all, y_all\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khc6APruwpUY",
        "outputId": "c7dfafd7-23db-4dae-b62d-f0d2fb44647a"
      },
      "source": [
        "\n",
        "def train_infer_bert():\n",
        "    (X_all, y_all) = get_training_data()\n",
        "\n",
        "    claims = [claim for (claim, _) in X_all]\n",
        "    evidences = [evidence for (_, evidence) in X_all]\n",
        "    print(\"Transforming claims\")\n",
        "    u = get_vectors(claims)\n",
        "    print(\"Transforming evidences\")\n",
        "    v = get_vectors(evidences)\n",
        "    print(\"Shape of u={} Shape of v={}\".format(u.shape, v.shape))\n",
        "    uplusv = u + v\n",
        "    uminusv = u - v\n",
        "    ubyv = u * v\n",
        "    print(\"Resulting shapes=\", uplusv.shape, uminusv.shape, ubyv.shape)\n",
        "    all_features = np.concatenate((u, v, uplusv, uminusv, ubyv), axis=1)\n",
        "    print(\"Shape of all=\", all_features.shape)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(all_features, y_all, test_size=0.2, random_state=4)\n",
        "    print(\"#Train=\", len(X_train), len(y_train))\n",
        "    print(\"#Test=\", len(X_test), len(y_test))\n",
        "    classifier_model_name = \"baseline_classifier_inferbert_model.h5\"\n",
        "    nn.EPOCHS=20\n",
        "    nn.fit_predict(X_train, y_train, X_test, y_test, classifier_model_name)\n",
        "\n",
        "train_infer_bert()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming claims\n",
            "Transforming evidences\n",
            "Shape of u=(250000, 768) Shape of v=(250000, 768)\n",
            "Resulting shapes= (250000, 768) (250000, 768) (250000, 768)\n",
            "Shape of all= (250000, 3840)\n",
            "#Train= 200000 200000\n",
            "#Test= 50000 50000\n",
            "Epoch 1/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.4847 - accuracy: 0.8838\n",
            "Epoch 2/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.3283 - accuracy: 0.9089\n",
            "Epoch 3/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.2895 - accuracy: 0.9205\n",
            "Epoch 4/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.2623 - accuracy: 0.9283\n",
            "Epoch 5/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.2386 - accuracy: 0.9356\n",
            "Epoch 6/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.2212 - accuracy: 0.9410\n",
            "Epoch 7/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.2047 - accuracy: 0.9462\n",
            "Epoch 8/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1907 - accuracy: 0.9506\n",
            "Epoch 9/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1818 - accuracy: 0.9534\n",
            "Epoch 10/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1736 - accuracy: 0.9559\n",
            "Epoch 11/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1641 - accuracy: 0.9585\n",
            "Epoch 12/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1575 - accuracy: 0.9607\n",
            "Epoch 13/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1502 - accuracy: 0.9628\n",
            "Epoch 14/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1445 - accuracy: 0.9644\n",
            "Epoch 15/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1386 - accuracy: 0.9662\n",
            "Epoch 16/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1344 - accuracy: 0.9673\n",
            "Epoch 17/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1295 - accuracy: 0.9691\n",
            "Epoch 18/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1246 - accuracy: 0.9704\n",
            "Epoch 19/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1220 - accuracy: 0.9712\n",
            "Epoch 20/20\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 0.1172 - accuracy: 0.9729\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 300)               1152300   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 903       \n",
            "=================================================================\n",
            "Total params: 1,157,409\n",
            "Trainable params: 1,157,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9400    0.9530    0.9465     25972\n",
            "           1     0.8704    0.8377    0.8537      9448\n",
            "           2     0.9380    0.9376    0.9378     14580\n",
            "\n",
            "    accuracy                         0.9267     50000\n",
            "   macro avg     0.9161    0.9095    0.9127     50000\n",
            "weighted avg     0.9262    0.9267    0.9264     50000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}