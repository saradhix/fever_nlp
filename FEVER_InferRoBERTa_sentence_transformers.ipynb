{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FEVER-InferRoBERTa-sentence-transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms91KhQZMMMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de5e75f-3476-4b01-b763-cfdf79d0c540"
      },
      "source": [
        "#Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSXsvdAiair6",
        "outputId": "92ca55cd-5ec3-4cdd-e185-964dfb469877"
      },
      "source": [
        "import os\n",
        "drive='/content/gdrive/My Drive/python/fever/'  #Change the path to the correct directory where the cleaned_data.csv file is present\n",
        "os.chdir(drive)\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/python/fever\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ntzcrUW9PWn",
        "outputId": "eef71bdd-7182-4e1c-bdd5-987dcf8ef582"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/5a/6e41e8383913dd2ba923cdcd02be2e03911595f4d2f9de559ecbed80d2d3/sentence-transformers-0.3.9.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n",
            "\u001b[?25hCollecting transformers<3.6.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 67.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.8)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 22.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers) (50.3.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.9-cp36-none-any.whl size=101036 sha256=9a0fb73c979ff876d2fc4e4f75a2bc5276c47a9df494bb5ce49a6e67505cdf1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/89/43/f2f5bc00b03ef9724b0f6254a97eaf159a4c4ddc024b33e07a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=1197470db42a44e8c4e9bbceb872ece78f5b595c7e7c1c1292e4b8752fbfb9ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.9 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xac_jLolxfBE",
        "outputId": "e9a66e87-4f66-42f8-d76a-d8c00d36f0cd"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B89_DtQIvdZN",
        "outputId": "b821f047-16fb-4202-871d-cd0da8866b8b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "body_index\t\t\t    neural_network.py\n",
            "cache_dir\t\t\t    __pycache__\n",
            "combined_replaced_pronouns.jsonl    runs\n",
            "fever_bert_base_encased\t\t    tfidf_vectorizer_5000_features.pickle\n",
            "formatted_data_train_3_class.jsonl  title_index\n",
            "license.html\t\t\t    wiki-pages\n",
            "__MACOSX\t\t\t    wiki-pages.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0ab6PCq9UxT"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "def get_vectors(sentences):\n",
        "  model_name = 'roberta-base-nli-stsb-mean-tokens'\n",
        "  model = SentenceTransformer(model_name)\n",
        "  embeddings = model.encode(sentences)\n",
        "  return embeddings \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBEHS19yx1Zx",
        "outputId": "ead511d5-72f2-430d-e000-8b8122c119ad"
      },
      "source": [
        "print(get_vectors([\"This is a cat\", \"This is a dog\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 461M/461M [00:08<00:00, 55.2MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0.88927007 -0.27933952  0.73015517 ... -0.36903322 -0.00148076\n",
            "   1.0066966 ]\n",
            " [ 0.10002015 -0.1111249  -0.67696    ...  0.5894578   0.37469926\n",
            "  -0.24015073]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs5GDWGsbqgs"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import neural_network as nn\n",
        "def pre_process(sentence):\n",
        "    ##Replace brackets\n",
        "    brackets = ['-LRB-', '-LSB-', '-RRB-', '-RSB-']\n",
        "    for bracket in brackets:\n",
        "        sentence = sentence.replace(bracket, \" \")\n",
        "    return sentence\n",
        "\n",
        "def get_training_data():\n",
        "    samples_to_use = 300000  #max=368892\n",
        "    label_dict = {\"SUPPORTS\":0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
        "\n",
        "    X_all = []\n",
        "    y_all = []\n",
        "    count = 0\n",
        "    #train_file = \"formatted_data_train.jsonl\"\n",
        "    train_file = \"formatted_data_train_3_class.jsonl\"\n",
        "    fp = open(train_file, 'r')\n",
        "    for line in fp:\n",
        "        if count >= samples_to_use:\n",
        "            break\n",
        "        obj = json.loads(line.strip())\n",
        "        claim = pre_process(obj['claim'])\n",
        "        evidence = pre_process(obj['evidence'])\n",
        "        X = (claim, evidence)\n",
        "        y = label_dict[obj['label']]\n",
        "        X_all.append(X)\n",
        "        y_all.append(y)\n",
        "        count += 1\n",
        "    return X_all, y_all\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khc6APruwpUY",
        "outputId": "dca97b64-4221-427a-cd25-590a3a3fb5b4"
      },
      "source": [
        "def train_infer_roberta():\n",
        "    (X_all, y_all) = get_training_data()\n",
        "\n",
        "    claims = [claim for (claim, _) in X_all]\n",
        "    evidences = [evidence for (_, evidence) in X_all]\n",
        "    print(\"Transforming claims\")\n",
        "    u = get_vectors(claims)\n",
        "    print(\"Transforming evidences\")\n",
        "    v = get_vectors(evidences)\n",
        "    print(\"Shape of u={} Shape of v={}\".format(u.shape, v.shape))\n",
        "    uplusv = u + v\n",
        "    uminusv = abs(u - v)\n",
        "    ubyv = u * v\n",
        "    print(\"Resulting shapes=\", uplusv.shape, uminusv.shape, ubyv.shape)\n",
        "    all_features = np.concatenate((u, v, uplusv, uminusv, ubyv), axis=1)\n",
        "    print(\"Shape of all=\", all_features.shape)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(all_features, y_all, test_size=0.2, random_state=4)\n",
        "    print(\"#Train=\", len(X_train), len(y_train))\n",
        "    print(\"#Test=\", len(X_test), len(y_test))\n",
        "    classifier_model_name = \"baseline_classifier_infer_roberta_model.h5\"\n",
        "    nn.EPOCHS=20\n",
        "    nn.fit_predict(X_train, y_train, X_test, y_test, classifier_model_name)\n",
        "\n",
        "train_infer_roberta()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transforming claims\n",
            "Transforming evidences\n",
            "Shape of u=(300000, 768) Shape of v=(300000, 768)\n",
            "Resulting shapes= (300000, 768) (300000, 768) (300000, 768)\n",
            "Shape of all= (300000, 3840)\n",
            "#Train= 240000 240000\n",
            "#Test= 60000 60000\n",
            "Epoch 1/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.3127 - accuracy: 0.9483\n",
            "Epoch 2/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1782 - accuracy: 0.9585\n",
            "Epoch 3/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1639 - accuracy: 0.9623\n",
            "Epoch 4/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1543 - accuracy: 0.9651\n",
            "Epoch 5/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1452 - accuracy: 0.9675\n",
            "Epoch 6/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1384 - accuracy: 0.9693\n",
            "Epoch 7/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1335 - accuracy: 0.9707\n",
            "Epoch 8/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1279 - accuracy: 0.9720\n",
            "Epoch 9/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1244 - accuracy: 0.9731\n",
            "Epoch 10/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1200 - accuracy: 0.9742\n",
            "Epoch 11/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1177 - accuracy: 0.9748\n",
            "Epoch 12/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1147 - accuracy: 0.9755\n",
            "Epoch 13/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1115 - accuracy: 0.9764\n",
            "Epoch 14/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1096 - accuracy: 0.9770\n",
            "Epoch 15/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1075 - accuracy: 0.9775\n",
            "Epoch 16/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1059 - accuracy: 0.9778\n",
            "Epoch 17/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1038 - accuracy: 0.9786\n",
            "Epoch 18/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1027 - accuracy: 0.9789\n",
            "Epoch 19/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1008 - accuracy: 0.9793\n",
            "Epoch 20/20\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0988 - accuracy: 0.9799\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 300)               1152300   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 903       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 3)                 903       \n",
            "=================================================================\n",
            "Total params: 1,161,615\n",
            "Trainable params: 1,161,615\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9541    0.9727    0.9633     31194\n",
            "           1     0.9325    0.8622    0.8960     11391\n",
            "           2     0.9647    0.9787    0.9716     17415\n",
            "\n",
            "    accuracy                         0.9534     60000\n",
            "   macro avg     0.9504    0.9378    0.9436     60000\n",
            "weighted avg     0.9531    0.9534    0.9529     60000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}